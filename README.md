
â¸»

ğŸ¤– LLM Social Simulation

A full-stack project for simulating social interactions between AI agents (LLMs).
Bots run locally using Ollamaï¿¼, and the system includes:
	â€¢	ğŸ§  Backend (FastAPI) â€” manages bot logic, simulation, and interactions
	â€¢	ğŸ’¬ Client Frontend (React) â€” displays social-media-style conversations
	â€¢	ğŸ§‘â€ğŸ’» Admin Dashboard (React) â€” lets you run simulations, ask specific bots questions, and view logs

â¸»

ğŸš€ Features
	â€¢	Multiple AI â€œbotsâ€ (each with a unique persona and model)
	â€¢	Local inference (no API keys required)
	â€¢	Round-based conversation simulation between bots
	â€¢	Ask individual bots direct questions
	â€¢	View and analyze conversation logs
	â€¢	Extensible architecture (easy to add models or personas)

â¸»

ğŸ§© Tech Stack

Component	Technology
Backend	Python + FastAPI
LLM Runtime	Ollama (local models)
Frontend (Client & Admin)	React + Vite + Tailwind
Data Storage	JSON logs
Communication	REST API


â¸»

ğŸ“¦ Installation & Setup

1ï¸âƒ£ Clone the Repository

git clone https://github.com/yourusername/llm-social-sim.git
cd llm-social-sim

2ï¸âƒ£ Install Ollama

Download and install from https://ollama.ai/downloadï¿¼

Verify installation:

ollama --version

3ï¸âƒ£ Pull Required Models

The default bots use these models (you can adjust in backend/bots/personas.json):

ollama pull mistral:7b

ollama pull llama3

ollama pull gemma:2b

Check installation:

ollama list

You should see:

mistral:7b   llama3   gemma:2b

ğŸŸ¢ Ensure the Ollama server is running in the background:

ollama serve


â¸»

ğŸ§  Backend Setup

Create a virtual environment:

python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate

Install dependencies:

pip install -r requirements.txt

Run the FastAPI server:

uvicorn backend.main:app --reload

Now open http://localhost:8000ï¿¼ to verify.

â¸»

ğŸŒ Frontend Setup

You have two frontends inside frontend/:

ğŸ§‘â€ğŸ¤â€ğŸ§‘ Client (Social Feed)

Displays the conversation feed between bots.

cd frontend/client
npm install
npm run dev

â†’ Runs on http://localhost:5173ï¿¼

â¸»

ğŸ§‘â€ğŸ’» Admin Dashboard

Control panel to:
	â€¢	Start new simulations
	â€¢	Ask specific bots questions
	â€¢	View latest logs and bot info

cd ../admin
npm install
npm run dev

â†’ Runs on http://localhost:5174ï¿¼

â¸»

ğŸ§° API Overview

Method	Endpoint	Description
GET	/	Health check
GET	/bots	List configured bots
POST	/simulate?rounds=5	Start a multi-bot conversation (use optional JSON body { "message": "Your topic" })
POST	/ask?bot=Clara	Ask a specific bot a direct question (send { "question": "Hi!" })
GET	/logs	Retrieve last saved conversation

Example (cURL)

curl -X POST "http://localhost:8000/simulate?rounds=3" \
     -H "Content-Type: application/json" \
     -d '{"message": "What do you think about AI in politics?"}'


â¸»

ğŸ§© Configuration

All bot personas are defined in:

backend/bots/personas.json

Example:

[
  {
    "name": "Clara",
    "model": "mistral:7b",
    "persona": "You are a progressive sociology student who values equality and climate action."
  },
  {
    "name": "Tom",
    "model": "llama3",
    "persona": "You are a pragmatic entrepreneur who values economic growth and innovation."
  }
]

You can add as many bots as you like â€” just make sure the model name matches one listed in ollama list.

â¸»

ğŸ“Š Logs

After each simulation, the conversation is saved in:

backend/data/logs/conversation.json

You can open this file or load it into a notebook (analysis/) for visualization or sentiment analysis.

â¸»

ğŸ”§ Troubleshooting

Problem	Solution
âŒ model not found	Run ollama pull <model> or check ollama list
âŒ Connection refused on port 11434	Run ollama serve
âš ï¸ React key warnings	Ensure <option key={bot.name}> in dropdowns
ğŸ§© 404 on /ask	Bot name not found in personas.json
ğŸ¤– Replies mention persona	Use system role in OllamaBot (already set in your code)


â¸»

ğŸ§ª Example Flow
	1.	Start backend (uvicorn backend.main:app --reload)
	2.	Start both frontends (npm run dev in each folder)
	3.	Open Admin Dashboard â†’ click â€œRun Simulationâ€
	4.	Open Client Feed â†’ see the conversation appear
	5.	In Admin, use â€œAsk a Botâ€ to query an individual agent
	6.	View logs or analyze results in /backend/data/logs/conversation.json

â¸»

ğŸ§­ Next Steps
	â€¢	Add new personas and models (Phi-3, Mixtral, Yi-Large, etc.)
	â€¢	Introduce topic selection per run
	â€¢	Visualize conversations (sentiment, reply network)
	â€¢	Optionally integrate a small database (SQLite, Firestore)

â¸»

ğŸ“„ License

MIT License â€” feel free to modify, extend, and use for research or teaching.

â¸»
